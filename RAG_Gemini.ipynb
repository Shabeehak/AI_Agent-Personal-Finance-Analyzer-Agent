{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shabeehak/AI_Agent-Personal-Finance-Analyzer-Agent/blob/main/RAG_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-01",
      "metadata": {
        "id": "cell-01"
      },
      "source": [
        "# üìÑ RAG ‚Äî Document Q&A with Generative AI\n",
        "### Project: Retrieval-Augmented Generation (RAG)\n",
        "**Model: Google Gemini 2.0 Flash (Free API) | New google-genai SDK**\n",
        "\n",
        "---\n",
        "\n",
        "## What is this project?\n",
        "\n",
        "This notebook demonstrates **RAG (Retrieval-Augmented Generation)**:\n",
        "\n",
        "1. Load a **text document** (your knowledge base)\n",
        "2. User asks a **question**\n",
        "3. System finds the **most relevant chunk** using TF-IDF similarity\n",
        "4. **Gemini LLM generates** a natural language answer from that chunk\n",
        "\n",
        "> üí° **Why RAG instead of fine-tuning?**  \n",
        "> Fine-tuning needs hours of GPU compute and costs money.  \n",
        "> RAG achieves the same result in seconds ‚Äî used by ChatGPT, Bing AI, and Google Gemini.\n",
        "\n",
        "---\n",
        "\n",
        "## RAG Pipeline\n",
        "\n",
        "```\n",
        "Document ‚îÄ‚îÄ‚ñ∫ Split into Chunks ‚îÄ‚îÄ‚ñ∫ TF-IDF Index\n",
        "                                        ‚îÇ\n",
        "Question ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Similarity Search\n",
        "                                        ‚îÇ\n",
        "                              Relevant Chunks\n",
        "                                        ‚îÇ\n",
        "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                    ‚îÇ  Gemini LLM generates the answer  ‚îÇ\n",
        "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## How to get a FREE Gemini API Key\n",
        "\n",
        "1. Go to üëâ https://aistudio.google.com/apikey\n",
        "2. Sign in with your Google account\n",
        "3. Click **Create API Key**\n",
        "4. Copy and paste it in **Step 2** below\n",
        "\n",
        "> ‚úÖ No credit card required ‚Äî free tier: 15 requests/min, 1500 requests/day"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-02",
      "metadata": {
        "id": "cell-02"
      },
      "source": [
        "## Step 1 ‚Äî Install & Import Libraries\n",
        "\n",
        "> ‚ö†Ô∏è Note: The old `google-generativeai` library is **deprecated** as of Nov 2025.  \n",
        "> We use the new official SDK: `google-genai`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-03",
      "metadata": {
        "id": "cell-03",
        "outputId": "7cb245cf-3a0d-415e-9491-bddf91eef23e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 26.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install the NEW official Google GenAI SDK + other libraries\n",
        "!pip install google-genai scikit-learn numpy --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-04",
      "metadata": {
        "id": "cell-04",
        "outputId": "2bf659bf-0b96-4a93-cc81-14cb32825999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì All libraries imported successfully\n",
            "  SDK: google-genai (new official SDK)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from google import genai                              # New SDK\n",
        "from google.genai import types                        # For config\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print('‚úì All libraries imported successfully')\n",
        "print('  SDK: google-genai (new official SDK)')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-05",
      "metadata": {
        "id": "cell-05"
      },
      "source": [
        "## Step 2 ‚Äî Set Your FREE Gemini API Key\n",
        "\n",
        "Get your free key from üëâ https://aistudio.google.com/apikey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da271211-f50e-40bf-850c-3bd15b8502ad",
      "metadata": {
        "id": "da271211-f50e-40bf-850c-3bd15b8502ad",
        "outputId": "9e277612-802c-4e5e-e44c-58ba685e18f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available Gemini models that support generateContent:\n",
            "\n",
            "  models/gemini-2.5-flash\n",
            "  models/gemini-2.5-pro\n",
            "  models/gemini-2.0-flash\n",
            "  models/gemini-2.0-flash-001\n",
            "  models/gemini-2.0-flash-exp-image-generation\n",
            "  models/gemini-2.0-flash-lite-001\n",
            "  models/gemini-2.0-flash-lite\n",
            "  models/gemini-2.5-flash-preview-tts\n",
            "  models/gemini-2.5-pro-preview-tts\n",
            "  models/gemma-3-1b-it\n",
            "  models/gemma-3-4b-it\n",
            "  models/gemma-3-12b-it\n",
            "  models/gemma-3-27b-it\n",
            "  models/gemma-3n-e4b-it\n",
            "  models/gemma-3n-e2b-it\n",
            "  models/gemini-flash-latest\n",
            "  models/gemini-flash-lite-latest\n",
            "  models/gemini-pro-latest\n",
            "  models/gemini-2.5-flash-lite\n",
            "  models/gemini-2.5-flash-image\n",
            "  models/gemini-2.5-flash-lite-preview-09-2025\n",
            "  models/gemini-3-pro-preview\n",
            "  models/gemini-3-flash-preview\n",
            "  models/gemini-3.1-pro-preview\n",
            "  models/gemini-3.1-pro-preview-customtools\n",
            "  models/gemini-3-pro-image-preview\n",
            "  models/nano-banana-pro-preview\n",
            "  models/gemini-robotics-er-1.5-preview\n",
            "  models/gemini-2.5-computer-use-preview-10-2025\n",
            "  models/deep-research-pro-preview-12-2025\n"
          ]
        }
      ],
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "\n",
        "print(\"Available Gemini models that support generateContent:\\n\")\n",
        "for model in client.models.list():\n",
        "    if 'generateContent' in model.supported_actions:\n",
        "        print(f\"  {model.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-06",
      "metadata": {
        "id": "cell-06",
        "outputId": "a91bca3b-7779-4840-fa2b-84e83c10159d"
      },
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "Enter Gemini API Key:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Gemini client ready\n",
            "‚úì Model: gemini-2.5-flash\n",
            "‚úì Free tier: 15 req/min, 1500 req/day\n"
          ]
        }
      ],
      "source": [
        "# Gemini API key here\n",
        "import getpass\n",
        "GEMINI_API_KEY = getpass.getpass(\"Enter Gemini API Key: \")\n",
        "\n",
        "# Initialize the new Gemini client\n",
        "client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "\n",
        "MODEL = 'gemini-2.5-flash'\n",
        "\n",
        "print(f'‚úì Gemini client ready')\n",
        "print(f'‚úì Model: {MODEL}')\n",
        "print(f'‚úì Free tier: 15 req/min, 1500 req/day')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-07",
      "metadata": {
        "id": "cell-07"
      },
      "source": [
        "## Step 3 ‚Äî Load the Document\n",
        "\n",
        "A sample LLM study document is ready to go.  \n",
        "Replace `DOCUMENT` with any text ‚Äî research paper, notes, textbook chapter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-08",
      "metadata": {
        "id": "cell-08",
        "outputId": "34fc2844-b991-41ce-c776-0e35c77415fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Document loaded: 382 words\n",
            "Preview: Large Language Models (LLMs) are AI models trained on massive amounts of text data.\n",
            "They learn patterns in language and can generate human-like text.\n",
            "Examples include GPT-4, Claude, Gemini, and LLaMA. ...\n"
          ]
        }
      ],
      "source": [
        "DOCUMENT = \"\"\"\n",
        "Large Language Models (LLMs) are AI models trained on massive amounts of text data.\n",
        "They learn patterns in language and can generate human-like text.\n",
        "Examples include GPT-4, Claude, Gemini, and LLaMA.\n",
        "\n",
        "Architecture: LLMs are based on the Transformer architecture introduced in the 2017 paper\n",
        "Attention Is All You Need. Transformers use self-attention to weigh the importance of\n",
        "different words when processing each word. The architecture has encoder and decoder layers\n",
        "with multiple attention heads that capture different relationships in text.\n",
        "\n",
        "Training: LLMs are trained using unsupervised pre-training on massive text datasets from\n",
        "the internet, books, and other sources. The model learns to predict the next word in a\n",
        "sequence. This forces the model to understand grammar, facts, and reasoning. Training\n",
        "requires thousands of GPUs running for weeks and costs millions of dollars.\n",
        "\n",
        "Fine-tuning: After pre-training, LLMs can be fine-tuned on specific tasks or datasets.\n",
        "This adapts the model for use cases like medical diagnosis or customer support.\n",
        "RLHF (Reinforcement Learning from Human Feedback) is a popular fine-tuning method\n",
        "used by ChatGPT to make models more helpful and safe.\n",
        "\n",
        "Advantages: LLMs can understand and generate text in many languages, perform zero-shot\n",
        "learning, follow complex instructions, write code, summarize documents, and hold\n",
        "multi-turn conversations. They are highly versatile across many industries.\n",
        "\n",
        "Disadvantages: LLMs can hallucinate, generating plausible but false information.\n",
        "They have a knowledge cutoff date. They are expensive to train and run.\n",
        "They can reflect biases in training data and struggle with precise math.\n",
        "\n",
        "RAG (Retrieval-Augmented Generation): RAG combines information retrieval with text\n",
        "generation. Instead of relying only on training knowledge, RAG fetches relevant documents\n",
        "at query time and includes them in the prompt. This lets LLMs answer questions about\n",
        "documents they were never trained on and reduces hallucination.\n",
        "\n",
        "LLM Comparison: GPT-4 by OpenAI is known for strong reasoning and code generation.\n",
        "Claude by Anthropic focuses on safety and long context windows. Gemini by Google is\n",
        "multimodal and integrates with Google services. LLaMA by Meta is open-source and free.\n",
        "Each model has different strengths, pricing, and context window sizes.\n",
        "\n",
        "Text Generation: LLMs generate text token by token. A token is roughly 4 characters.\n",
        "The model predicts the probability of the next token given all previous tokens.\n",
        "Temperature controls randomness ‚Äî low temperature gives predictable output,\n",
        "high temperature gives more creative and varied output.\n",
        "\"\"\"\n",
        "\n",
        "print(f'‚úì Document loaded: {len(DOCUMENT.split())} words')\n",
        "print('Preview:', DOCUMENT.strip()[:200], '...')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-09",
      "metadata": {
        "id": "cell-09"
      },
      "source": [
        "## Step 4 ‚Äî Split Document into Chunks\n",
        "\n",
        "We split the document into paragraphs. We only send the **most relevant chunks**  \n",
        "to Gemini ‚Äî not the whole document ‚Äî to stay within token limits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-10",
      "metadata": {
        "id": "cell-10",
        "outputId": "14ab4325-2a16-4331-ab8e-775d5a65ed97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Document split into 9 chunks\n",
            "\n",
            "Chunk 1: Large Language Models (LLMs) are AI models trained on massive amounts of text da...\n",
            "Chunk 2: Architecture: LLMs are based on the Transformer architecture introduced in the 2...\n",
            "Chunk 3: Training: LLMs are trained using unsupervised pre-training on massive text datas...\n",
            "Chunk 4: Fine-tuning: After pre-training, LLMs can be fine-tuned on specific tasks or dat...\n",
            "Chunk 5: Advantages: LLMs can understand and generate text in many languages, perform zer...\n",
            "Chunk 6: Disadvantages: LLMs can hallucinate, generating plausible but false information....\n",
            "Chunk 7: RAG (Retrieval-Augmented Generation): RAG combines information retrieval with te...\n",
            "Chunk 8: LLM Comparison: GPT-4 by OpenAI is known for strong reasoning and code generatio...\n",
            "Chunk 9: Text Generation: LLMs generate text token by token. A token is roughly 4 charact...\n"
          ]
        }
      ],
      "source": [
        "def split_into_chunks(text):\n",
        "    \"\"\"Split document into paragraph-level chunks.\"\"\"\n",
        "    paragraphs = [p.strip() for p in text.strip().split('\\n\\n') if p.strip()]\n",
        "    return paragraphs\n",
        "\n",
        "chunks = split_into_chunks(DOCUMENT)\n",
        "\n",
        "print(f'‚úì Document split into {len(chunks)} chunks\\n')\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f'Chunk {i+1}: {chunk[:80]}...')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-11",
      "metadata": {
        "id": "cell-11"
      },
      "source": [
        "## Step 5 ‚Äî Build TF-IDF Retrieval Index\n",
        "\n",
        "**TF-IDF** converts text into numbers (vectors) so we can measure similarity.\n",
        "\n",
        "| Term | Meaning |\n",
        "|------|---------|\n",
        "| TF (Term Frequency) | How often a word appears in a chunk |\n",
        "| IDF (Inverse Document Frequency) | How rare the word is across all chunks |\n",
        "| Cosine Similarity | Higher score = more relevant to the question |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-12",
      "metadata": {
        "id": "cell-12",
        "outputId": "c1fec65f-5568-4e07-fadc-81da8d308a5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì TF-IDF index built\n",
            "  Vocabulary size : 187 unique terms\n",
            "  Matrix shape    : (9, 187)  (chunks x terms)\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "chunk_vectors = vectorizer.fit_transform(chunks)\n",
        "\n",
        "print('‚úì TF-IDF index built')\n",
        "print(f'  Vocabulary size : {len(vectorizer.vocabulary_)} unique terms')\n",
        "print(f'  Matrix shape    : {chunk_vectors.shape}  (chunks x terms)')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-13",
      "metadata": {
        "id": "cell-13"
      },
      "source": [
        "## Step 6 ‚Äî Retrieval Function\n",
        "\n",
        "Finds the **top-N most relevant chunks** for any question using cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-14",
      "metadata": {
        "id": "cell-14",
        "outputId": "f99911dc-cfdb-48f0-d22d-a289e6ed02fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test: \"What is fine-tuning?\"\n",
            "\n",
            "Rank 1 [score: 0.5734]: Fine-tuning: After pre-training, LLMs can be fine-tuned on specific tasks or datasets.\n",
            "This adapts t...\n",
            "Rank 2 [score: 0.0]: Text Generation: LLMs generate text token by token. A token is roughly 4 characters.\n",
            "The model predi...\n",
            "Rank 3 [score: 0.0]: LLM Comparison: GPT-4 by OpenAI is known for strong reasoning and code generation.\n",
            "Claude by Anthrop...\n"
          ]
        }
      ],
      "source": [
        "def retrieve_chunks(question, top_n=3):\n",
        "    \"\"\"\n",
        "    Find most relevant chunks for a question.\n",
        "    1. Convert question to TF-IDF vector\n",
        "    2. Compute cosine similarity with all chunks\n",
        "    3. Return top N most similar chunks\n",
        "    \"\"\"\n",
        "    question_vector = vectorizer.transform([question])\n",
        "    similarities = cosine_similarity(question_vector, chunk_vectors)[0]\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_n]\n",
        "    return [{'chunk': chunks[i], 'score': round(float(similarities[i]), 4)} for i in top_indices]\n",
        "\n",
        "# Quick test\n",
        "test_results = retrieve_chunks('What is fine-tuning?')\n",
        "print('Test: \"What is fine-tuning?\"\\n')\n",
        "for i, r in enumerate(test_results):\n",
        "    print(f'Rank {i+1} [score: {r[\"score\"]}]: {r[\"chunk\"][:100]}...')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-15",
      "metadata": {
        "id": "cell-15"
      },
      "source": [
        "## Step 7 ‚Äî Full RAG Pipeline with Gemini ü§ñ\n",
        "\n",
        "```\n",
        "R ‚Äî Retrieve  ‚Üí TF-IDF finds relevant chunks\n",
        "A ‚Äî Augment   ‚Üí Chunks are injected into the prompt\n",
        "G ‚Äî Generate  ‚Üí Gemini LLM generates the answer\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-16",
      "metadata": {
        "id": "cell-16",
        "outputId": "c470b25e-d479-41e8-dd60-115208d3d463"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì RAG pipeline ready!\n"
          ]
        }
      ],
      "source": [
        "def ask(question, top_n=3):\n",
        "    \"\"\"\n",
        "    Full RAG Pipeline:\n",
        "    R - Retrieve relevant chunks from document\n",
        "    A - Augment the prompt with those chunks\n",
        "    G - Generate answer using Gemini LLM\n",
        "    \"\"\"\n",
        "    print('=' * 60)\n",
        "    print(f'QUESTION: {question}')\n",
        "    print('=' * 60)\n",
        "\n",
        "    # ‚îÄ‚îÄ R: Retrieve ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    relevant = retrieve_chunks(question, top_n=top_n)\n",
        "    context = '\\n\\n'.join([r['chunk'] for r in relevant])\n",
        "\n",
        "    print('\\nRELEVANT CHUNKS RETRIEVED:')\n",
        "    for i, r in enumerate(relevant):\n",
        "        print(f'  [{i+1}] score={r[\"score\"]} | {r[\"chunk\"][:80]}...')\n",
        "\n",
        "    # ‚îÄ‚îÄ A: Augment ‚Äî build the prompt ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    prompt = f\"\"\"You are an academic assistant. Answer the question using ONLY the context below.\n",
        "If the answer is not in the context, say: This information is not in the document.\n",
        "\n",
        "CONTEXT FROM DOCUMENT:\n",
        "{context}\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "ANSWER:\"\"\"\n",
        "\n",
        "    # ‚îÄ‚îÄ G: Generate ‚Äî call Gemini API (new SDK) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL,\n",
        "        contents=prompt,\n",
        "        config=types.GenerateContentConfig(\n",
        "            max_output_tokens=300,\n",
        "            temperature=0.3      # Low = focused, factual answers\n",
        "        )\n",
        "    )\n",
        "\n",
        "    print('\\nGEMINI GENERATED ANSWER:')\n",
        "    print('-' * 40)\n",
        "    print(response.text)\n",
        "    time.sleep(5)\n",
        "    print()\n",
        "\n",
        "print('‚úì RAG pipeline ready!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-17",
      "metadata": {
        "id": "cell-17"
      },
      "source": [
        "## Step 8 ‚Äî Ask Questions! üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-18",
      "metadata": {
        "id": "cell-18",
        "outputId": "583d8d66-1809-46f5-c2eb-bb8160a5c6ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "QUESTION: What is a Large Language Model?\n",
            "============================================================\n",
            "\n",
            "RELEVANT CHUNKS RETRIEVED:\n",
            "  [1] score=0.412 | Large Language Models (LLMs) are AI models trained on massive amounts of text da...\n",
            "  [2] score=0.0992 | Training: LLMs are trained using unsupervised pre-training on massive text datas...\n",
            "  [3] score=0.0458 | LLM Comparison: GPT-4 by OpenAI is known for strong reasoning and code generatio...\n",
            "\n",
            "GEMINI GENERATED ANSWER:\n",
            "----------------------------------------\n",
            "Large Language Models (LLMs) are AI models trained on massive amounts of text data.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ask('What is a Large Language Model?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-19",
      "metadata": {
        "id": "cell-19",
        "outputId": "0ee74782-b268-4a91-b501-af776b98a753"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "QUESTION: What are the advantages and disadvantages of LLMs?\n",
            "============================================================\n",
            "\n",
            "RELEVANT CHUNKS RETRIEVED:\n",
            "  [1] score=0.1897 | Disadvantages: LLMs can hallucinate, generating plausible but false information....\n",
            "  [2] score=0.175 | Advantages: LLMs can understand and generate text in many languages, perform zer...\n",
            "  [3] score=0.026 | Large Language Models (LLMs) are AI models trained on massive amounts of text da...\n",
            "\n",
            "GEMINI GENERATED ANSWER:\n",
            "----------------------------------------\n",
            "Advantages of LLMs include their ability to understand and generate text in many languages, perform zero-shot learning, follow complex instructions, write code, summarize documents, and hold multi-turn conversations. They are also highly versatile across many industries.\n",
            "\n",
            "Disadvantages of LLMs include their tendency to hallucinate (generating plausible but false information), having a knowledge cutoff date, being expensive to train and run, reflecting biases in training data, and struggling with precise math.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ask('What are the advantages and disadvantages of LLMs?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-20",
      "metadata": {
        "id": "cell-20",
        "outputId": "6e0a3ff3-ee04-4a63-db3f-9cf0c65504bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "QUESTION: How does RAG work and why is it better than fine-tuning?\n",
            "============================================================\n",
            "\n",
            "RELEVANT CHUNKS RETRIEVED:\n",
            "  [1] score=0.4682 | Fine-tuning: After pre-training, LLMs can be fine-tuned on specific tasks or dat...\n",
            "  [2] score=0.2893 | RAG (Retrieval-Augmented Generation): RAG combines information retrieval with te...\n",
            "  [3] score=0.0 | Text Generation: LLMs generate text token by token. A token is roughly 4 charact...\n",
            "\n",
            "GEMINI GENERATED ANSWER:\n",
            "----------------------------------------\n",
            "RAG combines information retrieval with text generation. It fetches\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ask('How does RAG work and why is it better than fine-tuning?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-21",
      "metadata": {
        "id": "cell-21",
        "outputId": "43980fb2-cd4b-46cf-df3e-1bda15c5a2bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "QUESTION: Compare GPT-4, Claude and Gemini\n",
            "============================================================\n",
            "\n",
            "RELEVANT CHUNKS RETRIEVED:\n",
            "  [1] score=0.3126 | Large Language Models (LLMs) are AI models trained on massive amounts of text da...\n",
            "  [2] score=0.2474 | LLM Comparison: GPT-4 by OpenAI is known for strong reasoning and code generatio...\n",
            "  [3] score=0.0 | Text Generation: LLMs generate text token by token. A token is roughly 4 charact...\n",
            "\n",
            "GEMINI GENERATED ANSWER:\n",
            "----------------------------------------\n",
            "GPT-4 by OpenAI is known for strong reasoning and code generation. Claude by Anthropic focuses on safety and long context windows. Gemini by Google is multimodal and integrates with Google services.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ask('Compare GPT-4, Claude and Gemini')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-22",
      "metadata": {
        "id": "cell-22"
      },
      "source": [
        "## Step 9 ‚Äî Ask Your Own Question üí¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-23",
      "metadata": {
        "id": "cell-23",
        "outputId": "4c56db8c-381c-4eb5-a09c-bc713257121c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "QUESTION: How does text generation work in LLMs?\n",
            "============================================================\n",
            "\n",
            "RELEVANT CHUNKS RETRIEVED:\n",
            "  [1] score=0.2567 | RAG (Retrieval-Augmented Generation): RAG combines information retrieval with te...\n",
            "  [2] score=0.1795 | Text Generation: LLMs generate text token by token. A token is roughly 4 charact...\n",
            "  [3] score=0.1547 | Large Language Models (LLMs) are AI models trained on massive amounts of text da...\n",
            "\n",
            "GEMINI GENERATED ANSWER:\n",
            "----------------------------------------\n",
            "LLMs generate text token by token. A token is roughly 4 characters. The model predicts the probability of the next token given all previous tokens. Temperature controls randomness; low temperature gives predictable output, while high temperature gives more creative and varied output.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Change the question and run!\n",
        "ask('How does text generation work in LLMs?')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-24",
      "metadata": {
        "id": "cell-24"
      },
      "source": [
        "## Step 10 ‚Äî Use Your Own Document üìÅ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-25",
      "metadata": {
        "id": "cell-25",
        "outputId": "ae469762-636c-4514-f836-ac1bfc1aff3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì New document loaded: 1 chunks ready\n"
          ]
        }
      ],
      "source": [
        "# Option A: Load from .txt file\n",
        "# with open('my_document.txt', 'r') as f:\n",
        "#     DOCUMENT = f.read()\n",
        "\n",
        "# Option B: Paste your own text\n",
        "DOCUMENT = \"\"\"\n",
        "A non-linear data structure is a data structure in which data elements are not arranged sequentially or linearly. Instead, elements can be connected to multiple other elements, forming complex relationships, hierarchies, or networks.\n",
        "Key Characteristics\n",
        "Non-sequential arrangement: Elements are arranged in random order and not in a straight line.\n",
        "Multiple connections: Each element (node) can be linked to more than one other element, allowing for multiple paths between nodes.\n",
        "Multi-level storage: Data elements are present at multiple levels, typically in a hierarchical manner.\n",
        "Complex traversal: Traversing all elements often requires specialized algorithms like Depth First Search (DFS) or Breadth First Search (BFS), and cannot be done in a single linear run.\n",
        "Efficient memory usage: Non-linear structures can utilize memory more efficiently by dynamically allocating space based on the data's structure, reducing memory wastage seen in some fixed-size linear structures like arrays.\n",
        "Common Examples and Applications\n",
        "The main types of non-linear data structures are trees and graphs.\n",
        "Trees\n",
        "Description: A hierarchical structure with a single root node at the top, and subsequent nodes organized in a parent-child relationship.\n",
        "Examples: Binary trees, Binary Search Trees (BST), AVL trees, B-trees, and heaps.\n",
        "Applications:\n",
        "Organizing file systems\n",
        "Organizational charts\n",
        "Indexing in databases\n",
        "Syntax trees in compilers\n",
        "Source: GeeksforGeeks, Naukri Code 360, CMU School of Computer Science\n",
        "Graphs\n",
        "Description: A collection of vertices (nodes) connected by edges, used to model relationships between entities.\n",
        "Examples: Directed graphs, undirected graphs, weighted graphs.\n",
        "Applications:\n",
        "Modeling social networks\n",
        "Transportation and road networks\n",
        "Mapping web pages and links (World Wide Web)\n",
        "Artificial Intelligence and image processing\n",
        "\"\"\"\n",
        "\n",
        "# Rebuild index with new document\n",
        "chunks = split_into_chunks(DOCUMENT)\n",
        "chunk_vectors = vectorizer.fit_transform(chunks)\n",
        "print(f'‚úì New document loaded: {len(chunks)} chunks ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-26",
      "metadata": {
        "id": "cell-26"
      },
      "source": [
        "## Summary\n",
        "\n",
        "| Task | Concept | Demonstrated Here |\n",
        "|------|---------|-------------------|\n",
        "| Task 1 | Text Generation | Gemini generates answers token by token |\n",
        "| Task 2 | Generative Model Working | `client.models.generate_content()` = text generation API |\n",
        "| Task 3 | Large Language Models | Used Gemini 2.0 Flash (free LLM) |\n",
        "| Task 4 | Architecture & Fine-tuning | RAG as alternative; `temperature` controls output |\n",
        "| Task 5 | LLM Comparison | Change `MODEL` to `gemini-2.0-flash` vs `gemini-1.5-pro` |\n",
        "\n",
        "---\n",
        "\n",
        "### RAG vs Fine-Tuning\n",
        "\n",
        "| | Fine-Tuning | RAG |\n",
        "|---|---|---|\n",
        "| Cost | Expensive ($$$) | Free ‚úÖ |\n",
        "| Time | Hours of GPU training | Seconds setup |\n",
        "| Update knowledge | Retrain the model | Just change the text |\n",
        "\n",
        "---\n",
        "### Key Formula\n",
        "```\n",
        "RAG = Retrieve (TF-IDF) + Augment (prompt) + Generate (Gemini LLM)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d641993-0b97-4a29-8d31-1cff71d3fd6d",
      "metadata": {
        "id": "8d641993-0b97-4a29-8d31-1cff71d3fd6d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}